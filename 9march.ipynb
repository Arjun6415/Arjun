{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38047a44-588d-4a05-9df0-7499908cf757",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac29a55-ac1d-43b9-b7ed-8bcc6a5bdb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "Probability Mass Function (PMF):\n",
    "The PMF is used for discrete random variables. It assigns a probability to each possible outcome of the random variable. In other words, it gives the probability that the random variable takes on a specific value.\n",
    "Mathematically, for a discrete random variable X, the PMF is denoted as P(X = x), where x represents a specific value the random variable can take.\n",
    "\n",
    "Example:\n",
    "Let's consider a fair six-sided die. The random variable X represents the outcome of a single roll of the die. The possible values of X are {1, 2, 3, 4, 5, 6}. Since the die is fair, each outcome has an equal probability of 1/6. The PMF for this scenario would be:\n",
    "\n",
    "P(X = 1) = 1/6\n",
    "P(X = 2) = 1/6\n",
    "P(X = 3) = 1/6\n",
    "P(X = 4) = 1/6\n",
    "P(X = 5) = 1/6\n",
    "P(X = 6) = 1/6\n",
    "\n",
    "Probability Density Function (PDF):\n",
    "The PDF is used for continuous random variables. Unlike discrete random variables, continuous random variables can take on an infinite number of values within a certain range. The PDF describes the relative likelihood of the random variable falling within a specific range of values.\n",
    "Mathematically, for a continuous random variable X, the PDF is denoted as f(x), and the probability that X lies within an interval [a, b] is given by the integral of the PDF over that interval:\n",
    "\n",
    "P(a ≤ X ≤ b) = ∫[a to b] f(x) dx\n",
    "\n",
    "Example:\n",
    "Consider a standard normal distribution, which is a commonly encountered continuous probability distribution. The random variable Z follows a standard normal distribution with a mean (μ) of 0 and a standard deviation (σ) of 1. The PDF of the standard normal distribution is given by the formula:\n",
    "\n",
    "f(z) = (1 / √(2π)) * e^(-z^2 / 2)\n",
    "\n",
    "Here, z represents a specific value on the standard normal distribution curve. The PDF gives us the relative likelihood of observing a particular value of z. Since the standard normal distribution is continuous, the PDF doesn't give the probability of specific values but rather the probability of values within a certain range.\n",
    "\n",
    "In summary, the PMF and PDF are fundamental concepts that help us understand the probabilities associated with discrete and continuous random variables, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804135db-58f4-433d-aa25-3a3399678cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f8c30c-a790-4da8-90a1-d3889dd33ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "The Cumulative Distribution Function (CDF) is a concept used in probability and statistics to describe the probability that a random variable takes on a value less than or equal to a given value. In other words, it provides information about the cumulative probability distribution of a random variable.\n",
    "\n",
    "Mathematically, for a random variable X, the CDF is denoted as F(x) and is defined as:\n",
    "\n",
    "F(x) = P(X ≤ x)\n",
    "\n",
    "The CDF gives us the probability that X falls within or below a certain value x.\n",
    "\n",
    "Example:\n",
    "Let's consider a fair six-sided die again. The random variable X represents the outcome of a single roll of the die. The possible values of X are {1, 2, 3, 4, 5, 6}. Since the die is fair, each outcome has an equal probability of 1/6.\n",
    "\n",
    "The CDF for this scenario would be:\n",
    "\n",
    "F(x) = P(X ≤ x)\n",
    "\n",
    "For x = 1:\n",
    "F(1) = P(X ≤ 1) = P(X = 1) = 1/6\n",
    "\n",
    "For x = 2:\n",
    "F(2) = P(X ≤ 2) = P(X = 1) + P(X = 2) = 1/6 + 1/6 = 1/3\n",
    "\n",
    "For x = 3:\n",
    "F(3) = P(X ≤ 3) = P(X = 1) + P(X = 2) + P(X = 3) = 1/6 + 1/6 + 1/6 = 1/2\n",
    "\n",
    "And so on...\n",
    "\n",
    "Why CDF is used:\n",
    "\n",
    "Cumulative Information: The CDF provides a comprehensive view of the distribution of a random variable by showing the probabilities of values up to a certain point. It summarizes how the probabilities accumulate as the values of the random variable increase.\n",
    "\n",
    "Probability Calculations: The CDF allows for straightforward calculations of probabilities involving ranges of values. For instance, if you want to know the probability that a random variable falls between two specific values, you can subtract the CDF values at those two points to get the desired probability.\n",
    "\n",
    "Percentiles and Quantiles: The CDF is used to determine percentiles and quantiles, which help in understanding how data is spread out. For example, the median (50th percentile) is the value at which the CDF reaches 0.5.\n",
    "\n",
    "Comparing Distributions: The CDF is a useful tool for comparing different probability distributions. It helps in visualizing and understanding how the distributions differ in terms of probabilities and percentiles.\n",
    "\n",
    "In summary, the Cumulative Distribution Function (CDF) provides a comprehensive view of the distribution of a random variable, making it easier to calculate probabilities, analyze percentiles, and compare different distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83905d02-589f-49f5-bb15-7c665cf5ca52",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c7807e-9b6a-454e-89bb-38ab44b85ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "The normal distribution, also known as the Gaussian distribution or bell curve, is a fundamental concept in statistics and probability theory. It is widely used to model a variety of natural and social phenomena due to its unique properties. Here are some examples of situations where the normal distribution might be used as a model:\n",
    "\n",
    "Height of Individuals: In many populations, human height tends to follow a normal distribution. The mean (average) height and the standard deviation (a measure of the spread) can be used to characterize the distribution of heights.\n",
    "\n",
    "Measurement Errors: When measuring physical quantities like length, weight, or temperature, small errors are often introduced. These errors can be modeled using a normal distribution, where the mean represents the true value and the standard deviation represents the measurement error.\n",
    "\n",
    "Test Scores: In educational assessment, test scores often approximate a normal distribution. The mean score and standard deviation can provide insights into how the scores are distributed among students.\n",
    "\n",
    "IQ Scores: Intelligence quotient (IQ) scores are often modeled using a normal distribution. The mean IQ score is typically set at 100, and the standard deviation helps describe the distribution of scores around the mean.\n",
    "\n",
    "Natural Phenomena: Many natural phenomena, like the distribution of particle velocities in a gas or the distribution of errors in scientific measurements, can be approximated by a normal distribution.\n",
    "\n",
    "Financial Data: Stock prices and financial returns often exhibit behavior that can be modeled using a normal distribution. This is a simplification, as financial markets can be more complex, but the normal distribution can provide a starting point.\n",
    "\n",
    "Biological Measurements: Biological traits like heart rate, blood pressure, and enzyme activity can often be modeled using a normal distribution.\n",
    "\n",
    "The parameters of the normal distribution are the mean (μ) and the standard deviation (σ). These parameters play a crucial role in defining the shape of the distribution:\n",
    "\n",
    "Mean (μ): The mean represents the central value around which the data cluster. It is also the peak of the bell curve. Shifting the mean to the right or left will cause the entire distribution to shift accordingly.\n",
    "\n",
    "Standard Deviation (σ): The standard deviation measures the spread or dispersion of the data points around the mean. A larger standard deviation leads to a wider, flatter curve, indicating more variability in the data. A smaller standard deviation results in a narrower, taller curve, indicating less variability.\n",
    "\n",
    "In summary, the normal distribution is a versatile tool used to model a wide range of phenomena. Its parameters, mean and standard deviation, define the central tendency and variability of the distribution, respectively, and play a significant role in shaping the curve that describes the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d1a589-7379-4170-be0b-b3c8b4b28830",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c1211b-eba9-46aa-9283-db2e5e502e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "The normal distribution holds significant importance in statistics and various fields due to its wide-ranging applicability and several mathematical properties. Here are some reasons why the normal distribution is important:\n",
    "\n",
    "Central Limit Theorem: One of the most critical aspects is the Central Limit Theorem. It states that the distribution of the sample means from any population, regardless of its original distribution, will tend to follow a normal distribution as the sample size increases. This theorem forms the basis for many statistical methods, allowing us to make inferences about a population based on sample data.\n",
    "\n",
    "Inference and Hypothesis Testing: The normal distribution is used as a foundation for hypothesis testing, confidence intervals, and significance testing. These techniques are vital for drawing conclusions from data and making informed decisions in various fields.\n",
    "\n",
    "Parametric Modeling: Many statistical models assume data is normally distributed. This assumption simplifies analyses and often leads to powerful insights. Linear regression, for example, assumes that the residuals (errors) of the model follow a normal distribution.\n",
    "\n",
    "Z-Scores and Standardization: Z-scores, obtained by standardizing data based on the normal distribution, allow us to compare and analyze values from different distributions on a common scale. This is helpful in comparing data points in various contexts.\n",
    "\n",
    "Risk Assessment and Probability: Many risk assessments and probability calculations rely on the normal distribution. Events like stock price movements, extreme weather conditions, and failure rates can often be modeled using this distribution.\n",
    "\n",
    "Quality Control: In manufacturing, the normal distribution is often used to monitor and control the quality of products. Deviations from the mean can indicate defects or errors.\n",
    "\n",
    "Population Studies: In social sciences and biology, traits like height, weight, and IQ scores are often distributed normally. Understanding these distributions helps in making informed decisions and predictions about populations.\n",
    "\n",
    "Psychometrics: In psychology, the normal distribution is used to model traits, behaviors, and test scores. This is critical for understanding and analyzing psychological phenomena.\n",
    "\n",
    "Real-life examples of normal distribution include:\n",
    "\n",
    "IQ Scores: Intelligence quotient scores tend to follow a normal distribution. Most people have an average IQ score, while fewer people have very low or very high scores.\n",
    "\n",
    "Height: Human height across a large population generally follows a normal distribution, with most individuals clustered around the average height.\n",
    "\n",
    "Exam Scores: In educational settings, the distribution of exam scores often approximates a normal distribution, with a peak around the average score.\n",
    "\n",
    "Body Temperature: The distribution of body temperatures in healthy individuals closely resembles a normal distribution, centered around the average body temperature.\n",
    "\n",
    "Random Measurement Errors: When conducting scientific experiments, measurement errors due to various factors often follow a normal distribution.\n",
    "\n",
    "Stock Market Returns: While stock returns are not perfectly normal, they often exhibit behavior that can be approximated by a normal distribution, forming the basis for risk assessment and portfolio management.\n",
    "\n",
    "The normal distribution's ubiquity and mathematical properties make it a foundational concept in statistics and various fields, enabling researchers, analysts, and decision-makers to make sense of complex data and draw meaningful insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6c5ba6-ae0e-4d82-ad2d-b15b790b384b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b0b0d6-42e7-4f17-ad4b-3abf48d50a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "The Bernoulli distribution is a discrete probability distribution that models a random experiment with two possible outcomes: \"success\" (usually denoted as 1) or \"failure\" (usually denoted as 0). It's named after Jacob Bernoulli, a Swiss mathematician. The distribution is characterized by a single parameter, often denoted as \"p,\" which represents the probability of success.\n",
    "\n",
    "Mathematically, the probability mass function (PMF) of a Bernoulli distribution is:\n",
    "\n",
    "P(X = x) = p^x * (1 - p)^(1 - x)\n",
    "\n",
    "Where:\n",
    "\n",
    "X is a random variable that can take values 0 or 1.\n",
    "x is the outcome of the random experiment (0 or 1).\n",
    "p is the probability of success (1) and (1 - p) is the probability of failure (0).\n",
    "An example of a Bernoulli distribution could be modeling the outcome of a single coin flip. Let's say we define \"success\" as getting heads and \"failure\" as getting tails. If the coin is fair, then the probability of getting heads (success) is 0.5, and the probability of getting tails (failure) is also 0.5.\n",
    "\n",
    "Now, moving on to the difference between the Bernoulli distribution and the Binomial distribution:\n",
    "\n",
    "Bernoulli Distribution:\n",
    "\n",
    "Deals with a single random experiment with two possible outcomes: success (1) or failure (0).\n",
    "The distribution is described by a single parameter, p, representing the probability of success.\n",
    "It models a single trial, like a single coin flip or a single test result (pass/fail).\n",
    "Binomial Distribution:\n",
    "\n",
    "Deals with a sequence of independent and identically distributed Bernoulli trials.\n",
    "It represents the number of successes in a fixed number (n) of independent trials, each with the same probability of success (p).\n",
    "The distribution is described by two parameters: n (number of trials) and p (probability of success).\n",
    "The random variable in a binomial distribution represents the count of successes in the given number of trials.\n",
    "In essence, the Bernoulli distribution is a special case of the binomial distribution where the number of trials (n) is 1. The binomial distribution generalizes this concept to multiple trials and allows us to calculate the probability of observing a specific number of successes in those trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bc7a62-f050-4127-9d3e-cb9340a56c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a1a32d-54f6-47d6-aff7-d99705987b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "To find the probability that a randomly selected observation from a normally distributed dataset with a mean of 50 and a standard deviation of 10 is greater than 60, we need to use the standard normal distribution (also known as the Z-distribution). This involves converting the value of 60 to its corresponding z-score and then finding the probability associated with that z-score.\n",
    "\n",
    "The formula to calculate the z-score is:\n",
    "�\n",
    "=\n",
    "�\n",
    "−\n",
    "�\n",
    "�\n",
    "z= \n",
    "σ\n",
    "x−μ\n",
    "​\n",
    " \n",
    "\n",
    "Where:\n",
    "\n",
    "�\n",
    "x is the value we want to find the probability for (in this case, 60).\n",
    "�\n",
    "μ is the mean of the dataset (given as 50).\n",
    "�\n",
    "σ is the standard deviation of the dataset (given as 10).\n",
    "Substituting the values:\n",
    "�\n",
    "=\n",
    "60\n",
    "−\n",
    "50\n",
    "10\n",
    "=\n",
    "1\n",
    "z= \n",
    "10\n",
    "60−50\n",
    "​\n",
    " =1\n",
    "\n",
    "Now, we want to find the probability that a randomly selected observation is greater than 60, which corresponds to the area under the standard normal distribution curve to the right of the z-score of 1.\n",
    "\n",
    "Using a standard normal distribution table or calculator, we can find that the probability associated with a z-score of 1 is approximately 0.8413.\n",
    "\n",
    "So, the probability that a randomly selected observation will be greater than 60 is approximately 0.8413, or 84.13%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31afccf7-e715-4258-84d3-6566fec5d0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12919d61-028f-402e-9ee1-82e798e8f779",
   "metadata": {},
   "outputs": [],
   "source": [
    "The uniform distribution is a type of probability distribution that describes a continuous random variable where all values within a specified range are equally likely to occur. In other words, in a uniform distribution, every possible outcome has the same probability of occurring.\n",
    "\n",
    "Imagine a situation where you roll a fair six-sided die. Each face of the die has an equal probability of landing face up, and this scenario can be represented by a uniform distribution.\n",
    "\n",
    "Let's define an example to illustrate the uniform distribution:\n",
    "\n",
    "Example:\n",
    "\n",
    "Suppose you have a random number generator that produces values between 0 and 1 with equal probability across that range. This random number generator follows a uniform distribution.\n",
    "\n",
    "In this case, the probability density function (PDF) of the uniform distribution can be described as:\n",
    "\n",
    "1, & \\text{if } 0 \\leq x \\leq 1 \\\\\n",
    "0, & \\text{otherwise}\n",
    "\\end{cases} \\]\n",
    "Here, \\( f(x) \\) represents the probability density of getting a value \\( x \\) from the uniform distribution.\n",
    "Now, let's say you want to find the probability that the random number generated falls between 0.2 and 0.7. Since the uniform distribution has a constant density within its range, the probability is simply the length of the interval (0.7 - 0.2) divided by the total range (1 - 0), which is:\n",
    "\\[ P(0.2 \\leq x \\leq 0.7) = \\frac{0.7 - 0.2}{1 - 0} = 0.5 \\]\n",
    "This makes sense because in a uniform distribution, every sub-interval within the range has an equal probability of containing the generated value.\n",
    "In summary, the uniform distribution represents scenarios where all outcomes within a given range are equally likely. It's often used in situations like random number generation or modeling scenarios where each possibility has the same likelihood of occurring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420971fa-163e-4b72-ae79-b8c5cb1c9a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99849dd7-3360-4ecb-886b-3936741ca0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "The z-score, also known as the standard score, is a statistical measure that quantifies how many standard deviations a data point is away from the mean of a dataset. It's used to standardize and compare values from different distributions, allowing you to understand the relative position of a data point within a distribution.\n",
    "\n",
    "Mathematically, the formula for calculating the z-score of a data point \n",
    "�\n",
    "x in a distribution with mean \n",
    "�\n",
    "μ and standard deviation \n",
    "�\n",
    "σ is:\n",
    "\n",
    "�\n",
    "=\n",
    "�\n",
    "−\n",
    "�\n",
    "�\n",
    "z= \n",
    "σ\n",
    "x−μ\n",
    "​\n",
    " \n",
    "\n",
    "Where:\n",
    "\n",
    "�\n",
    "x is the data point you want to convert to a z-score.\n",
    "�\n",
    "μ is the mean of the distribution.\n",
    "�\n",
    "σ is the standard deviation of the distribution.\n",
    "The importance of the z-score lies in its ability to provide insights into how unusual or typical a data point is in relation to the rest of the dataset. Here are some key uses and benefits of the z-score:\n",
    "\n",
    "Standardization and Comparison: The z-score standardizes data by converting it to a common scale based on the mean and standard deviation. This allows for easy comparison of data points from different distributions.\n",
    "\n",
    "Identification of Outliers: Data points with high or low z-scores are considered outliers. An outlier has a z-score that falls far from the mean, indicating that it's significantly different from the majority of the data points.\n",
    "\n",
    "Probability and Percentiles: Z-scores are often used to find the probability of a data point occurring within a certain range in a normal distribution. They are also used to determine the percentile rank of a data point within the distribution.\n",
    "\n",
    "Quality Control and Process Monitoring: In industries, z-scores are used to monitor and control processes. If a process produces values with z-scores that deviate too far from the expected range, it could indicate a problem in the process.\n",
    "\n",
    "Data Transformation: Z-scores can be used to transform data into a more normal distribution, which is useful in some statistical analyses and modeling techniques that assume normality.\n",
    "\n",
    "Research and Hypothesis Testing: Z-scores are used in hypothesis testing to determine whether observed data significantly deviates from expected values.\n",
    "\n",
    "Data Interpretation: Z-scores provide a standardized context for interpreting data. Positive z-scores indicate values above the mean, while negative z-scores indicate values below the mean.\n",
    "\n",
    "In summary, the z-score is a crucial tool in statistics that helps in comparing, analyzing, and interpreting data points within a distribution. It provides a standardized measure of how far a data point is from the mean, enabling better insights into the characteristics of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba37d51-5759-4ec7-a164-add23702a81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c9f845-cdcb-4ef2-872c-dd3e6f5fded9",
   "metadata": {},
   "outputs": [],
   "source": [
    "The Central Limit Theorem (CLT) is a fundamental concept in statistics that states that the distribution of the sample means of a sufficiently large number of independent and identically distributed random variables will be approximately normal, regardless of the original distribution of the individual variables. In simpler terms, it explains how the sampling distribution of the mean tends to become more and more normally distributed as the sample size increases, regardless of the underlying population distribution.\n",
    "\n",
    "The Central Limit Theorem is significant for several reasons:\n",
    "\n",
    "Normality Approximation: It allows us to assume that the sampling distribution of the mean from any population will be approximately normal if the sample size is large enough, regardless of the original population distribution. This is particularly important because many statistical methods and tests rely on the assumption of normality.\n",
    "\n",
    "Basis for Statistical Inference: The CLT forms the foundation for many statistical inference methods, such as hypothesis testing and confidence intervals. These methods depend on the assumption of normality, and the CLT justifies their use in practical scenarios.\n",
    "\n",
    "Small Sample Sizes: Even when dealing with small sample sizes, the CLT enables us to use normal distribution properties to make inferences about population parameters, as long as the sample size is reasonably large.\n",
    "\n",
    "Generalization to Diverse Populations: The CLT applies to a wide range of population distributions, even if the original distribution is not normal. This makes it a powerful tool for analyzing data from various sources.\n",
    "\n",
    "Data Analysis and Modeling: In practice, it's often difficult to know the underlying distribution of a population. The CLT allows statisticians to work with the means of samples, which tend to behave more predictably and can be modeled using the normal distribution.\n",
    "\n",
    "Sampling from Finite Populations: The CLT is also applicable when sampling from finite populations, as long as the sample size is small relative to the population size. This makes it useful for survey sampling and other practical applications.\n",
    "\n",
    "Explanation of Natural Phenomena: The phenomenon described by the CLT helps explain why many natural processes and measurements tend to exhibit a bell-shaped (normal) distribution, even if the individual components of the process don't follow a normal distribution.\n",
    "\n",
    "In summary, the Central Limit Theorem is a critical concept in statistics that underpins many statistical techniques and allows us to make robust inferences even when working with non-normally distributed data. It provides a bridge between the properties of individual observations and the behavior of sample means, enabling us to better understand and analyze complex data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fab47e-6ae9-49d9-a333-2fa8bce425a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c972f7b-ad9f-437b-b268-9ca0f0b464ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0970e0ad-f748-4141-83c5-597e0700fabe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
